# ğŸ“ Admission Chance Prediction  

---

## ğŸ“Œ Project Overview  
This project predicts the **chance of admission for graduate programs** based on studentsâ€™ academic and exam profiles.  
The dataset includes factors like GRE, TOEFL, university rating, SOP, LOR, CGPA, and research experience.  

We tested multiple regression models to see which one predicts admission chances most accurately.  

---

## ğŸ“‚ Dataset  
ğŸ“Š **Dataset Link:** [Graduate Admissions Dataset](https://www.kaggle.com/datasets/mohansacharya/graduate-admissions)  

- Shape â†’ **(400, 8)**  
- Clean dataset â†’ No duplicates  
- Dropped unnecessary columns  
- Train-test split: **80:20**  

---

## âš™ï¸ Tech Stack & Libraries  
- **Python** ğŸ  
- **NumPy, Pandas** â†’ Data handling  
- **Scikit-learn** â†’ ML models, RandomizedSearchCV  
- **Matplotlib / Seaborn** â†’ Visualization  

---

## ğŸš€ Models & Results  

| Model                     | RÂ² Score | Notes |
|----------------------------|----------|-------|
| ğŸŒ² Random Forest           | **0.81** | Strong baseline |
| Random Forest (RSCV)      | 0.75     | Drop after tuning (random nature) |
| âš¡ AdaBoost                | 0.802    | Good performance |
| AdaBoost (RSCV)           | 0.73     | Dropped after tuning |
| ğŸŒ± Gradient Boosting       | 0.73     | Underperformed |
| ğŸ“ˆ Linear Regression       | **0.82** âœ… | Best overall |
| Ridge Regression          | 0.82     | Same as LR |
| Lasso Regression          | 0.21 âŒ  | Performed very poorly |
| ğŸ¤ K-Nearest Neighbors     | 0.71     | Moderate |
| ğŸŒ³ Decision Tree           | 0.67     | Weakest |

---

## ğŸ’¡ Key Insights  
- **Linear Regression (0.82)** was the **best model**, showing the dataset is strongly linear in nature.  
- **Random Forest & AdaBoost** gave competitive results but not better than LR.  
- **Lasso collapsed (0.21)**, showing it over-penalized coefficients.  
- **Decision Trees** didnâ€™t generalize well, confirming weak non-linear relationships.  
- Ridge Regression confirmed that regularization didnâ€™t provide extra benefits over Linear Regression.  

---

## ğŸ¯ Conclusion  
Simple models worked best here.  
- **Linear Regression & Ridge Regression** â†’ Best and most reliable predictors.  
- Ensemble models like **Random Forest & AdaBoost** performed decently but were less consistent after hyperparameter tuning.  
- Lasso and Decision Trees showed significant underperformance.  

This project highlights that **not every dataset needs complex models â€” sometimes linear methods are the best fit.** âœ…  

---
